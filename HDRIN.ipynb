{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5266eb06",
   "metadata": {},
   "source": [
    "# 导入库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "344a5c8a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow.python.keras.backend'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtextblob\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TextBlob\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mK\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwordcloud\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WordCloud\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow.python.keras.backend'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from textblob import TextBlob\n",
    "import tensorflow.python.keras.backend as K\n",
    "from wordcloud import WordCloud\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import os, re\n",
    "from tensorflow.python.keras.layers import Flatten, Dense, Dropout, GlobalAveragePooling1D\n",
    "from tensorflow.python.keras.models import Model\n",
    "# from transformers import *\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.model_selection import GroupKFold, KFold\n",
    "import tensorflow_hub as hub\n",
    "# import sentencepiece\n",
    "# import tokenization\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "# import tensorflow_text as text\n",
    "from sklearn.model_selection import train_test_split\n",
    "# from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e57fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.test.is_gpu_available()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc37a724",
   "metadata": {},
   "source": [
    "# chinese model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61fd0be",
   "metadata": {
    "code_folding": [
     4,
     19,
     38,
     153
    ]
   },
   "outputs": [],
   "source": [
    "from keras.layers import Layer\n",
    "from tensorflow.keras import initializers, regularizers, constraints, optimizers, layers, callbacks\n",
    "from tensorflow.keras.layers import Input, Embedding, Conv1D, MaxPooling1D, GlobalMaxPooling1D, Dense, Activation, add,MultiHeadAttention\n",
    "from keras_self_attention import SeqSelfAttention\n",
    "def dot_product(x, kernel):\n",
    "    \"\"\"\n",
    "    Wrapper for dot product operation, in order to be compatible with both\n",
    "    Theano and Tensorflow\n",
    "    Args:\n",
    "        x (): input\n",
    "        kernel (): weights\n",
    "    Returns:\n",
    "    \"\"\"\n",
    "    if K.backend() == 'tensorflow':\n",
    "        return K.squeeze(K.dot(x, K.expand_dims(kernel)), axis=-1)\n",
    "    else:\n",
    "        return K.dot(x, kernel)\n",
    "def create_custom_objects():\n",
    "    instance_holder = {\"instance\": None}\n",
    "\n",
    "    class ClassWrapper(AttentionWithContext):\n",
    "        def __init__(self, *args, **kwargs):\n",
    "            instance_holder[\"instance\"] = self\n",
    "            super(ClassWrapper, self).__init__(*args, **kwargs)\n",
    "\n",
    "    def loss(*args):\n",
    "        method = getattr(instance_holder[\"instance\"], \"loss_function\")\n",
    "        return method(*args)\n",
    "\n",
    "    def accuracy(*args):\n",
    "        method = getattr(instance_holder[\"instance\"], \"accuracy\")\n",
    "        return method(*args)\n",
    "    return {\"ClassWrapper\": ClassWrapper ,\"AttentionWithContext\": ClassWrapper, \"loss\": loss,\n",
    "            \"accuracy\":accuracy}\n",
    "class AttentionWithContext(Layer):\n",
    "    \"\"\"\n",
    "    Attention operation, with a context/query vector, for temporal data.\n",
    "    Supports Masking.\n",
    "    Follows the work of Yang et al. [https://www.cs.cmu.edu/~diyiy/docs/naacl16.pdf]\n",
    "    \"Hierarchical Attention Networks for Document Classification\"\n",
    "    by using a context vector to assist the attention\n",
    "    # Input shape\n",
    "        3D tensor with shape: `(samples, steps, features)`.\n",
    "    # Output shape\n",
    "        2D tensor with shape: `(samples, features)`.\n",
    "    How to use:\n",
    "    Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n",
    "    The dimensions are inferred based on the output shape of the RNN.\n",
    "    Note: The layer has been tested with Keras 2.0.6\n",
    "    Example:\n",
    "        model.add(LSTM(64, return_sequences=True))\n",
    "        model.add(AttentionWithContext())\n",
    "        # next add a Dense layer (for classification/regression) or whatever...\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 W_regularizer=None, u_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, u_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.u_regularizer = regularizers.get(u_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.u_constraint = constraints.get(u_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        super(AttentionWithContext, self).__init__(**kwargs)\n",
    "        \n",
    "#     def get_config(self):\n",
    "#         config = super().get_config()\n",
    "#         config.update({\n",
    "#             \"arg1\": self.arg1,\n",
    "#             \"arg2\": self.arg2,\n",
    "#         })\n",
    "#         return config     \n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config().copy()\n",
    "        config.update({\n",
    "                'W_regularizer': self.W_regularizer,\n",
    "                'u_regularizer': self.u_regularizer,\n",
    "                'b_regularizer': self.b_regularizer,\n",
    "                'W_constraint': self.W_constraint,\n",
    "                'u_constraint': self.u_constraint,\n",
    "                'b_constraint': self.b_constraint,\n",
    "                'bias': self.bias,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight(shape=(input_shape[-1], input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight(shape=(input_shape[-1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "\n",
    "        self.u = self.add_weight(shape=(input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_u'.format(self.name),\n",
    "                                 regularizer=self.u_regularizer,\n",
    "                                 constraint=self.u_constraint)\n",
    "\n",
    "        super(AttentionWithContext, self).build(input_shape)\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        # do not pass the mask to the next layers\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        uit = dot_product(x, self.W)\n",
    "\n",
    "        if self.bias:\n",
    "            uit += self.b\n",
    "\n",
    "        uit = K.tanh(uit)\n",
    "        ait = dot_product(uit, self.u)\n",
    "\n",
    "        a = K.exp(ait)\n",
    "\n",
    "        # apply mask after the exp. will be re-normalized next\n",
    "        if mask is not None:\n",
    "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        # in some cases especially in the early stages of training the sum may be almost zero\n",
    "        # and this results in NaN's. A workaround is to add a very small positive number ε to the sum.\n",
    "        # a /= K.cast(K.sum(a, axis=1, keepdims=True), K.floatx())\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0], input_shape[-1]\n",
    "class ParallelCoAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super(ParallelCoAttention, self).__init__()\n",
    "        self.W_text = tf.keras.layers.Dense(units)\n",
    "        self.W_conv = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, sequence_output, conv_output):\n",
    "        # 计算文本序列和卷积输出之间的相似性\n",
    "        score_text = self.V(tf.nn.tanh(self.W_text(sequence_output)))\n",
    "        score_conv = self.V(tf.nn.tanh(self.W_conv(conv_output)))\n",
    "        \n",
    "        # 计算注意力权重\n",
    "        attention_weights_text = tf.nn.softmax(score_text, axis=1)\n",
    "        attention_weights_conv = tf.nn.softmax(score_conv, axis=1)\n",
    "\n",
    "        # 对文本序列和卷积输出进行注意力加权\n",
    "        context_text = tf.reduce_sum(attention_weights_text * sequence_output, axis=1)\n",
    "        context_conv = tf.reduce_sum(attention_weights_conv * conv_output, axis=1)\n",
    "\n",
    "        return context_text, context_conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92bace6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hugging face 代理使用\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import os\n",
    "os.environ['CURL_CA_BUNDLE'] = ''\n",
    "os.environ[\"http_proxy\"] = \"http://127.0.0.1:7890\"\n",
    "os.environ[\"https_proxy\"] = \"http://127.0.0.1:7890\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25296352",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, TFBertModel\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-chinese\")\n",
    "model = TFBertModel.from_pretrained(\"bert-base-chinese\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0148e879",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./textrankfinal.csv',index_col=0)  #Zhccvi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4776336",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "data = np.load('zhoutput_tensors.npy')\n",
    "labels = np.load('zhlabels.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3292ad34",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 0.2\n",
    "# 使用train_test_split进行划分\n",
    "train_data, test_data, train_labels, test_labels = train_test_split(data, labels, test_size=test_size, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74bb8554",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Concatenate, Dense, Dropout, Conv1D, MaxPooling1D, Reshape\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score,precision_score,recall_score,accuracy_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d5e505",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_scores = []\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "accuracy_scores = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029427ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_bertCA():\n",
    "    text_input = tf.keras.layers.Input(shape=(256, 768), dtype=tf.float32) \n",
    "    # 扩展维度以适应 AttentionWithContext 的要求\n",
    "    attention_output = AttentionWithContext()(text_input)\n",
    "    # 假设conv_output是卷积层的输出\n",
    "    conv_output = Conv1D(filters=32, kernel_size=3, padding=\"valid\", activation='relu')(text_input)\n",
    "    conv_output = MaxPooling1D(pool_size=2)(conv_output)\n",
    "    conv_output = GlobalAveragePooling1D()(conv_output)\n",
    "    # 定义并行注意力机制层\n",
    "    co_attention = ParallelCoAttention(units=64)\n",
    "    context_text, context_conv = co_attention(conv_output, attention_output)\n",
    "    context_text = tf.expand_dims(context_text, axis=1)  # 扩展维度\n",
    "    context_conv = tf.expand_dims(context_conv, axis=1)  # 扩展维度\n",
    "    pooler_output = tf.reduce_mean(text_input, axis=1)\n",
    "    merged = tf.keras.layers.concatenate([context_text, context_conv,pooler_output], axis=1)\n",
    "    fc1 = Dense(64, activation='relu', )(merged)\n",
    "    dropout1 = Dropout(0.3)(fc1)\n",
    "    fc2 = Dense(32, activation='relu', kernel_regularizer=tf.keras.regularizers.l1(0.1))(dropout1)\n",
    "    dropout2 = Dropout(0.3)(fc2)\n",
    "    net = tf.keras.layers.Dense(1, activation='sigmoid', name='classifier')(dropout2)\n",
    "    return tf.keras.Model(text_input, net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9abd025",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_folds = 5\n",
    "kf = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b49ab89",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 2e-5\n",
    "learning_decay = 1e-4 # 1e-4\n",
    "for train_index, test_index in kf.split(data, labels):\n",
    "    train_data, test_data = data[train_index], data[test_index]\n",
    "    train_labels, test_labels = labels[train_index], labels[test_index]\n",
    "    callbacks = [EarlyStopping(monitor='val_loss', patience=2,mode=\"min\",)]#当验证集的连续两个epoch指标不再改善时停止训练\n",
    "    # Build a new BERT model for each fold\n",
    "    bertCA = build_bertCAko()\n",
    "    bertCA.compile(tf.keras.optimizers.Adam(learning_rate=learning_rate, decay=learning_decay), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    bertCA.fit(train_data, train_labels, epochs=10, batch_size=16,validation_split=0.1,callbacks=callbacks, verbose=1)\n",
    "    predictions = bertCA.predict(test_data)\n",
    "    binary_predictions = (predictions > 0.5).astype(int)\n",
    "    f1 = f1_score(test_labels, binary_predictions)\n",
    "    precision = precision_score(test_labels, binary_predictions)\n",
    "    recall = recall_score(test_labels, binary_predictions)\n",
    "    accuracy = accuracy_score(test_labels, binary_predictions)\n",
    "    f1_scores.append(f1)\n",
    "    precision_scores.append(precision)\n",
    "    recall_scores.append(recall)\n",
    "    accuracy_scores.append(accuracy)\n",
    "# Calculate the average accuracy across all folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625fef6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "average_accuracy = sum(accuracy_scores) / num_folds\n",
    "average_f1_scores = sum(f1_scores) / num_folds\n",
    "average_precision_scores = sum(precision_scores) / num_folds\n",
    "average_recall_scores = sum(recall_scores) / num_folds\n",
    "\n",
    "print(f'Average Accuracy: {average_accuracy}')\n",
    "print(f'Average f1_scores: {average_f1_scores}')\n",
    "print(f'Average precision_scores: {average_precision_scores}')\n",
    "print(f'Average recall_scores: {average_recall_scores}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0548721c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac3c542",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3781fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955d7fa9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8e0f0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d276a1b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c738322",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8118dcb0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056e359a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f941e44c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3834c09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56475ea7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd3dcc2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "419px",
    "width": "211px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
